{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wcc-j1lbV_Gx",
        "outputId": "83b2f88c-4019-46d2-be46-29bc3e10c178"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=e70f4315ebdcad4c89c88d78cb281a6dffdac89353cc08d15b1d4854b271d297\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "import string\n",
        "from pyspark.sql.functions import desc\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize Spark\n",
        "conf = SparkConf().setAppName(\"BasicWordCount\")\n",
        "sc = SparkContext.getOrCreate(conf=conf)\n",
        "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n"
      ],
      "metadata": {
        "id": "QhIVu35TWc5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define stopwords\n",
        "STOPWORDS = set([\"the\", \"a\", \"and\", \"of\", \"in\", \"to\", \"is\", \"it\", \"for\", \"with\", \"on\", \"at\", \"by\", \"an\"])"
      ],
      "metadata": {
        "id": "VdvpMTZMXC1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to clean text\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    return text"
      ],
      "metadata": {
        "id": "U4TpEEplXCjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to process the file and count words\n",
        "def process_file(file_path):\n",
        "    # Read the text file into an RDD\n",
        "    rdd = sc.textFile(file_path)\n",
        "\n",
        "    # Process the text\n",
        "    words = (rdd.flatMap(lambda line: clean_text(line).split())  # Split lines into words\n",
        "             .filter(lambda word: word not in STOPWORDS and len(word) > 0)  # Remove stopwords and empty words\n",
        "             .map(lambda word: (word, 1))  # Map words to (word, 1)\n",
        "             .reduceByKey(lambda a, b: a + b)  # Reduce by key to count occurrences\n",
        "             .sortBy(lambda x: -x[1]))  # Sort by count in descending order\n",
        "\n",
        "    # Convert RDD to DataFrame\n",
        "    words_df = spark.createDataFrame(words, [\"word\", \"count\"])\n",
        "    words_df = words_df.sort(desc(\"count\"))\n",
        "\n",
        "    # Show top 25 results\n",
        "    words_df.show(25, truncate=False)\n",
        "\n",
        "    # Stop Spark session\n",
        "    spark.stop()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = \"/content/shakespeare.txt\"\n",
        "    process_file(file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twrFslXpXCJc",
        "outputId": "d02cb107-8393-4727-b139-3119b599d8f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+\n",
            "|word |count|\n",
            "+-----+-----+\n",
            "|my   |393  |\n",
            "|i    |344  |\n",
            "|that |322  |\n",
            "|thy  |287  |\n",
            "|thou |235  |\n",
            "|not  |167  |\n",
            "|me   |164  |\n",
            "|but  |163  |\n",
            "|love |162  |\n",
            "|thee |162  |\n",
            "|so   |145  |\n",
            "|be   |142  |\n",
            "|as   |121  |\n",
            "|all  |117  |\n",
            "|you  |112  |\n",
            "|which|108  |\n",
            "|his  |107  |\n",
            "|when |106  |\n",
            "|this |105  |\n",
            "|your |100  |\n",
            "|doth |88   |\n",
            "|do   |84   |\n",
            "|from |82   |\n",
            "|no   |79   |\n",
            "|or   |79   |\n",
            "+-----+-----+\n",
            "only showing top 25 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# Extended Word Count\"\"\"\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import desc\n",
        "import string\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ExtendedWordCount\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext  # Get SparkContext from SparkSession\n",
        "\n",
        "# Define stopwords\n",
        "STOPWORDS = set([\"the\", \"a\", \"and\", \"of\", \"in\", \"to\", \"is\", \"it\", \"for\", \"with\", \"on\", \"at\", \"by\", \"an\"])\n",
        "\n",
        "# Function to clean text\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    return text\n",
        "\n",
        "# Function to process multiple files and count words\n",
        "def process_files(file_paths):\n",
        "    # Create an RDD for all files\n",
        "    rdd = sc.textFile(\",\".join(file_paths))\n",
        "\n",
        "    # Clean and process the text\n",
        "    words = (rdd.flatMap(lambda line: clean_text(line).split())  # Split lines into words\n",
        "             .filter(lambda word: word not in STOPWORDS and len(word) > 0)  # Remove stopwords and empty words\n",
        "             .map(lambda word: (word, 1))  # Map words to (word, 1)\n",
        "             .reduceByKey(lambda a, b: a + b)  # Reduce by key to count occurrences\n",
        "             .sortBy(lambda x: -x[1]))  # Sort by count in descending order\n",
        "\n",
        "    # Convert RDD to DataFrame\n",
        "    words_df = spark.createDataFrame(words, [\"word\", \"count\"])\n",
        "    words_df = words_df.sort(desc(\"count\"))\n",
        "\n",
        "    # Show top 25 results\n",
        "    words_df.show(25, truncate=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    file_paths = [\"/content/text1.txt\", \"/content/text2.txt\"]  # List of file paths\n",
        "    process_files(file_paths)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Qq81JBeXCBV",
        "outputId": "c9bcfd91-fab0-49a5-e6eb-850bc2afef83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----+\n",
            "|word        |count|\n",
            "+------------+-----+\n",
            "|intelligence|3    |\n",
            "|machines    |3    |\n",
            "|artificial  |2    |\n",
            "|these       |2    |\n",
            "|machine     |2    |\n",
            "|learning    |2    |\n",
            "|models      |2    |\n",
            "|human       |1    |\n",
            "|are         |1    |\n",
            "|programmed  |1    |\n",
            "|think       |1    |\n",
            "|like        |1    |\n",
            "|ai          |1    |\n",
            "|science     |1    |\n",
            "|aiming      |1    |\n",
            "|smart       |1    |\n",
            "|become      |1    |\n",
            "|essential   |1    |\n",
            "|technology  |1    |\n",
            "|involves    |1    |\n",
            "|use         |1    |\n",
            "|algorithms  |1    |\n",
            "|statistical |1    |\n",
            "|allow       |1    |\n",
            "|computers   |1    |\n",
            "+------------+-----+\n",
            "only showing top 25 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "import string\n",
        "from pyspark.sql.functions import desc\n",
        "\n",
        "# Initialize Spark\n",
        "conf = SparkConf().setAppName(\"ExtendedWordCount\")\n",
        "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
        "\n",
        "# Define stopwords\n",
        "STOPWORDS = set([\"the\", \"a\", \"and\", \"of\", \"in\", \"to\", \"is\", \"it\", \"for\", \"with\", \"on\", \"at\", \"by\", \"an\"])\n",
        "\n",
        "# Function to clean text\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    return text\n",
        "\n",
        "# Function to process multiple files and count words\n",
        "def process_files(file_paths):\n",
        "    # Create an RDD for all files\n",
        "    rdd = sc.textFile(\",\".join(file_paths))\n",
        "\n",
        "    # Process the text\n",
        "    words = (rdd.flatMap(lambda line: clean_text(line).split())  # Split lines into words\n",
        "             .filter(lambda word: word not in STOPWORDS and len(word) > 0)  # Remove stopwords and empty words\n",
        "             .map(lambda word: (word, 1))  # Map words to (word, 1)\n",
        "             .reduceByKey(lambda a, b: a + b)  # Reduce by key to count occurrences\n",
        "             .sortBy(lambda x: -x[1]))  # Sort by count in descending order\n",
        "\n",
        "    # Convert RDD to DataFrame\n",
        "    words_df = spark.createDataFrame(words, [\"word\", \"count\"])\n",
        "    words_df = words_df.sort(desc(\"count\"))  # Sort DataFrame by count in descending order\n",
        "\n",
        "    # Show top 25 results\n",
        "    words_df.show(25, truncate=False)\n",
        "\n",
        "# Run the processing on your documents\n",
        "file_paths = [\"/content/doc1.txt\", \"/content/doc2.txt\"]\n",
        "process_files(file_paths)\n",
        "\n",
        "import subprocess\n",
        "\n",
        "# Command to run spark-submit\n",
        "command = [\"spark-submit\", \"extended_word_count.py\"]\n",
        "\n",
        "# Execute the command\n",
        "subprocess.run(command)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Utqn5_7-XB6q",
        "outputId": "a6def068-66fa-458c-ae32-0fe792de9e23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----+\n",
            "|word       |count|\n",
            "+-----------+-----+\n",
            "|word       |15   |\n",
            "|or         |7    |\n",
            "|words      |7    |\n",
            "|text       |6    |\n",
            "|count      |5    |\n",
            "|may        |5    |\n",
            "|be         |4    |\n",
            "|when       |3    |\n",
            "|used       |3    |\n",
            "|counts     |3    |\n",
            "|details    |3    |\n",
            "|variations |3    |\n",
            "|as         |3    |\n",
            "|are        |3    |\n",
            "|space      |3    |\n",
            "|sources    |3    |\n",
            "|definitions|3    |\n",
            "|processing |3    |\n",
            "|this       |3    |\n",
            "|definition |3    |\n",
            "|how        |3    |\n",
            "|consensus  |3    |\n",
            "|from       |2    |\n",
            "|counting   |2    |\n",
            "|also       |2    |\n",
            "+-----------+-----+\n",
            "only showing top 25 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CompletedProcess(args=['spark-submit', 'extended_word_count.py'], returncode=2)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WdF8dj-OXB0G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}